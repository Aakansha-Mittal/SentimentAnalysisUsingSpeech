{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa724b4-d352-4cef-a1ca-45dca23a163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting resampyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akans\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from resampy) (1.26.4)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\akans\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from resampy) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\akans\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.53->resampy) (0.43.0)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 2.4/3.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.1/3.1 MB 10.6 MB/s eta 0:00:00\n",
      "Installing collected packages: resampy\n",
      "Successfully installed resampy-0.4.3\n"
     ]
    }
   ],
   "source": [
    "pip install resampy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310b0c5e-6fdd-4b3f-be67-092641f6d461",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File C:\\Users\\akans\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mfccs_scaled\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m X_ravdess, y_ravdess \u001b[38;5;241m=\u001b[39m \u001b[43mload_ravdess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mravdess_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m X_tess, y_tess \u001b[38;5;241m=\u001b[39m load_tess_data(tess_path)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Combine RAVDESS and TESS datasets\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 38\u001b[0m, in \u001b[0;36mload_ravdess_data\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     36\u001b[0m         emotion \u001b[38;5;241m=\u001b[39m emotion_dict_ravdess\u001b[38;5;241m.\u001b[39mget(emotion_code)\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m emotion:\n\u001b[1;32m---> 38\u001b[0m             X\u001b[38;5;241m.\u001b[39mappend(\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     39\u001b[0m             y\u001b[38;5;241m.\u001b[39mappend(emotion)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "Cell \u001b[1;32mIn[5], line 56\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(file_path):\n\u001b[1;32m---> 56\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkaiser_fast\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     mfccs \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39msample_rate, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[0;32m     58\u001b[0m     mfccs_scaled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mfccs\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Take the mean of the MFCC features\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:193\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    190\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:678\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[0;32m    670\u001b[0m         soxr\u001b[38;5;241m.\u001b[39mresample,\n\u001b[0;32m    671\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    675\u001b[0m         quality\u001b[38;5;241m=\u001b[39mres_type,\n\u001b[0;32m    676\u001b[0m     )\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 678\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m(y, orig_sr, target_sr, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mres_type, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[0;32m    681\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, size\u001b[38;5;241m=\u001b[39mn_samples, axis\u001b[38;5;241m=\u001b[39maxis, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lazy_loader\\__init__.py:117\u001b[0m, in \u001b[0;36mDelayedImportErrorModule.__getattr__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__frame_data\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis error is lazily reported, having originally occured in\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  File \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(fd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_context\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    122\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'resampy'\n\nThis error is lazily reported, having originally occured in\n  File C:\\Users\\akans\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py, line 33, in <module>\n\n----> resampy = lazy.load(\"resampy\")"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Path to RAVDESS and TESS datasets\n",
    "ravdess_path = 'C:/Users/akans/OneDrive/Desktop/data/ravdess'\n",
    "tess_path = 'C:/Users/akans/OneDrive/Desktop/data/tess'\n",
    "\n",
    "# Emotion mapping for RAVDESS\n",
    "emotion_dict_ravdess = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Load RAVDESS dataset\n",
    "def load_ravdess_data(path):\n",
    "    X, y = [], []\n",
    "    for folder in os.listdir(path):\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            emotion_code = file.split(\"-\")[2]  # Emotion code is at position 3\n",
    "            emotion = emotion_dict_ravdess.get(emotion_code)\n",
    "            if emotion:\n",
    "                X.append(extract_features(file_path))\n",
    "                y.append(emotion)\n",
    "    return X, y\n",
    "\n",
    "# Load TESS dataset\n",
    "def load_tess_data(path):\n",
    "    X, y = [], []\n",
    "    for folder in os.listdir(path):\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            emotion = file.split(\"_\")[1].replace(\".wav\", \"\")  # Extract emotion from filename\n",
    "            X.append(extract_features(file_path))\n",
    "            y.append(emotion)\n",
    "    return X, y\n",
    "\n",
    "# Feature extraction using MFCCs\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    mfccs_scaled = np.mean(mfccs.T, axis=0)  # Take the mean of the MFCC features\n",
    "    return mfccs_scaled\n",
    "\n",
    "# Load data\n",
    "X_ravdess, y_ravdess = load_ravdess_data(ravdess_path)\n",
    "X_tess, y_tess = load_tess_data(tess_path)\n",
    "\n",
    "# Combine RAVDESS and TESS datasets\n",
    "X = np.array(X_ravdess + X_tess)\n",
    "y = np.array(y_ravdess + y_tess)\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for class imbalance\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "print(dict(zip(unique, counts)))  # Display the class distribution\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Print confusion matrix and classification report\n",
    "print(confusion_matrix(y_true, y_pred_classes))\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "\n",
    "# Save the trained model\n",
    "model.save('emotion_recognition_model_fixed.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855e0456-59f0-4504-a4b6-77a034a1d736",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sounddevice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msounddevice\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load RAVDESS dataset and extract MFCC features\n",
    "def load_ravdess_data(ravdess_path):\n",
    "    X, y = [], []\n",
    "    for actor in os.listdir(ravdess_path):\n",
    "        actor_path = os.path.join(ravdess_path, actor)\n",
    "        for file in os.listdir(actor_path):\n",
    "            file_path = os.path.join(actor_path, file)\n",
    "            emotion_code = int(file.split('-')[2])\n",
    "            if emotion_code in range(1, 9):\n",
    "                emotion_label = emotion_code - 1  # 0-indexed labels\n",
    "                y.append(emotion_label)\n",
    "                audio, sr = librosa.load(file_path, sr=None)\n",
    "                mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "                mfcc = np.mean(mfcc.T, axis=0)\n",
    "                X.append(mfcc)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load TESS dataset and extract MFCC features\n",
    "def load_tess_data(tess_path):\n",
    "    X, y = [], []\n",
    "    for folder in os.listdir(tess_path):\n",
    "        folder_path = os.path.join(tess_path, folder)\n",
    "        emotion_label = folder.split('_')[-1]\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            y.append(emotion_label)\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "            mfcc = np.mean(mfcc.T, axis=0)\n",
    "            X.append(mfcc)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Model creation\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Voice recording, processing and emotion prediction\n",
    "def predict_from_voice(model, classes):\n",
    "    duration = 3  # Recording duration in seconds\n",
    "    sr = 22050  # Sample rate\n",
    "    print(\"Recording for 3 seconds...\")\n",
    "    voice_input = sd.rec(int(duration * sr), samplerate=sr, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    voice_input = voice_input.flatten()\n",
    "    \n",
    "    mfcc = librosa.feature.mfcc(y=voice_input, sr=sr, n_mfcc=13)\n",
    "    mfcc = np.mean(mfcc.T, axis=0).reshape(1, -1)  # Reshape for model input\n",
    "    \n",
    "    prediction = model.predict(mfcc)\n",
    "    predicted_label = classes[np.argmax(prediction)]\n",
    "    print(f\"Predicted emotion: {predicted_label}\")\n",
    "\n",
    "# Main code\n",
    "ravdess_path = 'path_to_ravdess'\n",
    "tess_path = 'path_to_tess'\n",
    "\n",
    "# Load data\n",
    "X_ravdess, y_ravdess = load_ravdess_data(ravdess_path)\n",
    "X_tess, y_tess = load_tess_data(tess_path)\n",
    "\n",
    "# Encode TESS labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_tess = label_encoder.fit_transform(y_tess)\n",
    "\n",
    "# Combine datasets\n",
    "X = np.vstack((X_ravdess, X_tess))\n",
    "y = np.concatenate((y_ravdess, y_tess))\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train model\n",
    "input_shape = (X_train.shape[1],)\n",
    "num_classes = len(np.unique(y))\n",
    "model = create_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model and save training history\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Save the model\n",
    "model.save('emotion_recognition_model.h5')\n",
    "\n",
    "# Voice input and prediction\n",
    "classes = label_encoder.classes_  # Define emotion classes for prediction\n",
    "\n",
    "def run_emotion_detection():\n",
    "    predict_from_voice(model, classes)\n",
    "\n",
    "# To use the voice input feature, simply call:\n",
    "run_emotion_detection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adf2b6f-e656-45f9-8914-7efd77902e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_emotion_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_emotion_detection\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'run_emotion_detection' is not defined"
     ]
    }
   ],
   "source": [
    "run_emotion_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb60bd-9aed-42b2-bd0e-fcc60e35086e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
